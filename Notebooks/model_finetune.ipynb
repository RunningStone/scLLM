{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scLLM.Models.scBERT.model import PerformerLM\n",
    "from scLLM.Models.scBERT.paras import scBERT_para\n",
    "\n",
    "\n",
    "model_para = scBERT_para()    \n",
    "model_para.num_tokens=5+2                         # num of tokens\n",
    "model_para.max_seq_len=16906+1#24447+1              # max length of sequence\n",
    "model_para.dim=200                                # dim of tokens\n",
    "model_para.depth=6                              # layers\n",
    "model_para.heads=10\n",
    "model_para.local_attn_heads = 0 \n",
    "model_para.g2v_position_emb = True \n",
    "model_para.g2v_weight_loc = \"Path/to/gene2vec_16906_200.npy\"\n",
    "#\"/Users/shipan/Documents/scLLM_workspace/scLLM/pre_trained/scBERT/gene2vec_24447_200.npy\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cls_nb = 20\n",
    "upper_bound = 5\n",
    "ckpt_pth = \"/Path/to/panglao_pretrain.pth\"\n",
    "EPOCHS=10\n",
    "GRADIENT_ACCUMULATION=8\n",
    "LEARNING_RATE = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model from embedding file with saved numpy file\n",
    "#gene2vec_emb_loc = \"github/Gene2vec/pre_trained_emb/gene2vec_dim_200_iter_9_w2v.txt\"\n",
    "#from scLLM.Models.scBERT.utils import transfer_gene2vec_as_weight\n",
    "# get model from embedding file with saved numpy file\n",
    "#gs_model = transfer_gene2vec_as_weight(gene2vec_emb_loc,model_para.g2v_weight_loc)\n",
    "# get model from embedding file without saved numpy file\n",
    "#gs_model = transfer_gene2vec_as_weight(gene2vec_emb_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PerformerLM(model_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pre_model = torch.load(ckpt_pth, map_location=device)\n",
    "model.load_state_dict(pre_model[\"model_state_dict\"])\n",
    "\n",
    "# change output layer\n",
    "from scLLM.Modules.out_layer import Identity\n",
    "model.to_out = Identity(in_dim=model_para.max_seq_len,\n",
    "                        dropout=0., \n",
    "                        h_dim=128, \n",
    "                        out_dim=cls_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.norm.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.performer.net.layers[-2].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_loc = \"/Users/shipan/Documents/scLLM_workspace/pre_trained/scBERT/vocab_gene2vec_16906.pkl\"\n",
    "adata_loc = \"/Users/shipan/Documents/scLLM_workspace/data/Eloise/allMCF.h5ad\"\n",
    "\n",
    "import pickle\n",
    "with open(vocab_loc, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "# init preprocessor\n",
    "from scLLM.Dataset.preprocessor import Preprocessor\n",
    "from scLLM.Dataset.paras import Dataset_para\n",
    "# define pre-processing by follow original implementation of scBERT\n",
    "dataset_para = Dataset_para(gene_vocab=vocab,\n",
    "                            filter_gene_by_counts=False,\n",
    "                            filter_cell_by_counts=200,\n",
    "                            log1p=True,\n",
    "                            log1p_base=2,\n",
    "                            batch_size=1,\n",
    "                            )\n",
    "\n",
    "preprocess = Preprocessor(dataset_para)\n",
    "preprocess.load_adata(adata_loc)\n",
    "data = preprocess.to_data(data_type=\"log1p\")\n",
    "label,class_weight = preprocess.to_label(\n",
    "                          label_key=\"pseudotimes\",\n",
    "                          binarize=\"equal_instance\",\n",
    "                          bin_nb=cls_nb,)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=2023)\n",
    "\n",
    "idx_tr,idx_val = next(iter(sss.split(data, label)))\n",
    "data_train, label_train = data[idx_tr], label[idx_tr]\n",
    "data_val, label_val = data[idx_val], label[idx_val]\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "weights_train = [class_weight[label_train[i]] for i in range(label_train.shape[0])]\n",
    "sampler_train = WeightedRandomSampler(torch.DoubleTensor(weights_train), len(weights_train))\n",
    "weights_val = [class_weight[label_val[i]] for i in range(label_val.shape[0])]\n",
    "sampler_val = WeightedRandomSampler(torch.DoubleTensor(weights_val), len(weights_val))\n",
    "\n",
    "\n",
    "from scLLM.Dataset.dataset import SCDataset\n",
    "train_dataset = SCDataset(data_train, label_train,cls_nb=cls_nb,device=device)\n",
    "val_dataset = SCDataset(data_val, label_val,cls_nb=cls_nb,device=device)\n",
    "train_loader = preprocess.to_dataloader(train_dataset, sampler=sampler_train)\n",
    "val_loader = preprocess.to_dataloader(val_dataset,sampler=sampler_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingWarmRestarts, CyclicLR\n",
    "from scLLM.Models.scBERT.utils import CosineAnnealingWarmupRestarts\n",
    "\n",
    "# optimizer\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = CosineAnnealingWarmupRestarts(\n",
    "    optimizer,\n",
    "    first_cycle_steps=15,\n",
    "    cycle_mult=2,\n",
    "    max_lr=LEARNING_RATE,\n",
    "    min_lr=1e-6,\n",
    "    warmup_steps=5,\n",
    "    gamma=0.9\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss(weight=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1, EPOCHS+1):\n",
    "    #train_loader.sampler.set_epoch(i)\n",
    "    model.train()\n",
    "    #dist.barrier()\n",
    "    running_loss = 0.0\n",
    "    cum_acc = 0.0\n",
    "    for index, (data, labels) in enumerate(train_loader):\n",
    "        index += 1\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        if index % GRADIENT_ACCUMULATION != 0:\n",
    "            #with model.no_sync():\n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "        if index % GRADIENT_ACCUMULATION == 0:\n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e6))\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        final = softmax(logits)\n",
    "        final = final.argmax(dim=-1)\n",
    "        pred_num = labels.size(0)\n",
    "        correct_num = torch.eq(final, labels).sum(dim=-1)\n",
    "        cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "    epoch_loss = running_loss / index\n",
    "    epoch_acc = 100 * cum_acc / index\n",
    "\n",
    "    print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==')\n",
    "    \n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Histo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d54c460629d4e98d9a88def0a4b8f729ffa99ca7bfc826c2a753ee2b4ef8936"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
